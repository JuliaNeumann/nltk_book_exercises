{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dishes'[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'running'[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'nationality'[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'undo'[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'preheat'[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16dacbda7e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;34m'in'\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "'in'[-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pto'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "monty[6:11:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'otP'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[10:5:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[1:10:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-563dffb3b494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmonty\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'monty' is not defined"
     ]
    }
   ],
   "source": [
    "monty[1:6:1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nohtyP ytnoM'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk, re, pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Monty} {Python}\n"
     ]
    }
   ],
   "source": [
    "# a - one or more letters\n",
    "nltk.re_show(r'[a-zA-Z]+', monty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Monty} {Python}\n",
      "{A} very {Intersting}3 example\n"
     ]
    }
   ],
   "source": [
    "# b - one capital letter and zero or more lowercase letters\n",
    "nltk.re_show(r'[A-Z][a-z]*', monty)\n",
    "nltk.re_show(r'[A-Z][a-z]*', 'A very Intersting3 example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two {pout}ing party {pet}s - {pt}\n"
     ]
    }
   ],
   "source": [
    "# c - a word starting with p, followed by 0 up to 2 vowels and ending with p\n",
    "nltk.re_show(r'p[aeiou]{,2}t', 'two pouting party pets - pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This should match {23} as well as {1.093} and {999.9}\n"
     ]
    }
   ],
   "source": [
    "# d - integer or decimal number\n",
    "nltk.re_show(r'\\d+(\\.\\d+)?', 'This should match 23 as well as 1.093 and 999.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}T{his} {}s{}h{}o{}u{}l{}d{} {mat}c{}h{} {pet as} {wel}l{ as} {cut an}d{} {lol}\n"
     ]
    }
   ],
   "source": [
    "# e - zero or more sequences of not-a-vowel - vowel - not-a-vowel\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', 'This should match pet as well as cut and lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{should} {match} {me} {but} {not}\n"
     ]
    }
   ],
   "source": [
    "# f - one or more alphanumeric characters or one or more charcters that are neither alpahnumeric nor whitespace\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', 'should match me but not \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the something\n",
      "{the}\n",
      "{an}\n",
      "anything\n"
     ]
    }
   ],
   "source": [
    "a = r'^(the|a|an)$'\n",
    "nltk.re_show(a, 'the something')\n",
    "nltk.re_show(a, 'the')\n",
    "nltk.re_show(a, 'an')\n",
    "nltk.re_show(a, 'anything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something+2\n",
      "{2*3+8}\n",
      "{200+5000}\n",
      "{2*3+8}-5/6\n"
     ]
    }
   ],
   "source": [
    "b = r'\\d+([\\+\\*]\\d+)+'\n",
    "nltk.re_show(b, 'something+2')\n",
    "nltk.re_show(b, '2*3+8')\n",
    "nltk.re_show(b, '200+5000')\n",
    "nltk.re_show(b, '2*3+8-5/6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file c:\\python27\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'\\n\\n\\n\\nNatural Language Toolkit \\u2014 NLTK 3.0 documentation\\n\\n\\n\\n      var DOCUMENTATION_OPTIONS = {\\n        URL_ROOT:    \\'./\\',\\n        VERSION:     \\'3.0\\',\\n        COLLAPSE_INDEX: false,\\n        FILE_SUFFIX: \\'.html\\',\\n        HAS_SOURCE:  true\\n      };\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNLTK 3.0 documentation\\n\\nnext |\\n          modules |\\n          index\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNatural Language Toolkit\\xb6\\nNLTK is a leading platform for building Python programs to work with human language data.\\nIt provides easy-to-use interfaces to over 50 corpora and lexical\\nresources such as WordNet,\\nalong with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\\nwrappers for industrial-strength NLP libraries,\\nand an active discussion forum.\\nThanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\\nNLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\\nNLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\\nNLTK has been called \\u201ca wonderful tool for teaching, and working in, computational linguistics using Python,\\u201d\\nand \\u201can amazing library to play with natural language.\\u201d\\nNatural Language Processing with Python provides a practical\\nintroduction to programming for language processing.\\nWritten by the creators of NLTK, it guides the reader through the fundamentals\\nof writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\\nand more.\\nThe book is being updated for Python 3 and NLTK 3.\\n(The original Python 2 version is still available at http://nltk.org/book_1ed.)\\n\\nSome simple things you can do with NLTK\\xb6\\nTokenize and tag some text:\\n>>> import nltk\\n>>> sentence = \"\"\"At eight o\\'clock on Thursday morning\\n... Arthur didn\\'t feel very good.\"\"\"\\n>>> tokens = nltk.word_tokenize(sentence)\\n>>> tokens\\n[\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\',\\n\\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\']\\n>>> tagged = nltk.pos_tag(tokens)\\n>>> tagged[0:6]\\n[(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'),\\n(\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')]\\n\\n\\nIdentify named entities:\\n>>> entities = nltk.chunk.ne_chunk(tagged)\\n>>> entities\\nTree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'),\\n           (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'),\\n       Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]),\\n           (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'),\\n           (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')])\\n\\n\\nDisplay a parse tree:\\n>>> from nltk.corpus import treebank\\n>>> t = treebank.parsed_sents(\\'wsj_0001.mrg\\')[0]\\n>>> t.draw()\\n\\n\\n\\nNB. If you publish work that uses NLTK, please cite the NLTK book as\\nfollows:\\n\\nBird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O\\u2019Reilly Media Inc.\\n\\n\\nNext Steps\\xb6\\n\\nsign up for release announcements\\njoin in the discussion\\n\\n\\n\\n\\nContents\\xb6\\n\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\n\\nIndex\\nModule Index\\nSearch Page\\n\\n\\n\\n\\n\\n\\n\\nTable Of Contents\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n                Enter search terms or a module, class or function name.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnext |\\n            modules |\\n            index\\n\\n\\n\\nShow Source\\n\\n\\n\\n\\n        \\xa9 Copyright 2015, NLTK Project.\\n      Last updated on Apr 09, 2016.\\n      Created using Sphinx 1.3.1.\\n    \\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "def getContentFromURL(url):\n",
    "    raw = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(raw)\n",
    "    return soup.get_text()\n",
    "\n",
    "getContentFromURL('http://www.nltk.org/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load(fileName):\n",
    "    f = open(fileName + '.txt')\n",
    "    return f.read()\n",
    "corpusText = load('corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', ':', '-', \"'\", '-', '(', ')', ',', '.', '.', '.', '-', '.', '-', '-', '.', '.', '.', '.', '.', ':', '-', '-', ':', '-', '.', '-', ',', ',', ',', ',', '.', ',', '-', ',', ',', '.', '-', '-', ':', '-', ',', '-', ',', '-', '-', ',', '.', '-', '-', '.', '-', '-', ',', '-', '.', ',', '.', ',', ',', '-', ',', '.', ',', '-', '.', \"'\", '.', \"'\", '.', ',', '.', '.', ',', ',', '.', ',', '.', ',', '.', ',', '.', ',', ',', ',', ',', '.', ',', ',', ',', ',', '.', ',', '.', ',', ',', \"'\", \"'\", '(', ')', '.', \"'\", ',', '.', '-', '.', '.', ',', ',', '.', '.', '.', \"'\", \"'\", '.', \"'\", \"'\", '-', '.', '.', ',', '.', ',', ',', '(', ').', ',', '-', '.', \"'\", \"'\", ',', ',', '(', ',', ')', \"'\", '.', ',', ',', '-', '-', ',', '.', ',', '.', '-', '-', '.', ',', ',', ',', '.', '-', ',', ',', ',', ',', '(', '_', '_', '.', ')', '.', '[', '].', ',', ',', ',', '.', ',', '(', '.', ')', ',', '(', '_', '.', ')', '.', ',', '(', '_', '.', ')', '-', '.', '.', ',', ',', ',', '(', ').', ',', '.', '.', '.', ',', ':', ',', \"'\", \"',\", \"'\", \"',\", \"'\", \"',\", \"'\", \"'\", ',', '.', \"('\", \"',\", \"'\", \"',\", \"'\", \"'),\", \"('\", \"',\", \"'\", \"',\", \"'\", \"',\", \"'\", \"')\", '.', ',', ',', ',', ',', '.', \"'\", \"'\", ',', '.', ',', ',', ',', '.', ',', ',', '.', '-', '-', ',', '.', ',', ',', ',', '.', \"'\", \"'\", ',', \"'\", \"'\", '.', '.', ',', ',', \"'\", \"'\", \"'\", \"',\", ',', ',', '.', ',', '.', ',', '(', '-', '-', ')', '.', ',', ',', \"'\", '.', '.', '(', ',', ')', '(', ',', ',', ',', ',', ',', ',', '.),', '-', '-', '(', ')', '.', ',', '-', \"'\", \"'\", ',', \"'\", \"'\", ',', \"'\", \"'\", ',', '[', '].', '-', ':', '(', '_', '.', ')', \"'\", '_', \"',\", \"'\", \"',\", \"'\", \"',\", \"'\", \"'.\", '(', ')', '(', ',', ',', '-', ').', ',', ':', ',', \"'\", '_', \"'\", ',', \"'\", \"'\", ',', \"'\", \"'.\", ',', ',', '-', \"'\", \"'\", '(', '-', \"'\", \"'\", ',', '-', '-', ',', \"'\", \"'\", \"'\", \"'\", ').', '-', '.', ',', ',', \"'\", \"'\", ',', '.', ',', ',', ',', \"('\", \"',\", \"'\", \"'\", '.).', '.', '(', '.', '.', ',', '),', ',', '.', ',', '.', '.', ',', '.', ',', ',', '.', ',', '.', '(', '_', '.', ')', '-', '.', ',', ',', '.', ':', ',', ',', '(', ').', ',', '(', ',', ').', ',', '.', ',', '(', ')', ',', '.', ',', '(', ').', ',', ',', '.', '.', ',', '(', '),', '.', '(', '),', '.', ',', '(', '),', ':', ',', '.', ',', '.', ',', ',', '.', ',', ',', '.', ',', ',', '(', ')', '(', ').', ',', ',', ',', '[', ']', '.', ',', ',', '.', ',', ',', ',', ',', '.', ',', ',', '.', ',', ',', '.', ',', '.', ',', '(', ').', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', ',', '.', '.', '-', '-', ',', ',', '.', '-', ',', ',', '.', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', ':', ',', ',', '(', '),', '.', ',', ',', '(', '),', '.', ',', ',', '(', ',', ').', ',', '-', ',', '.', ',', '.', '-', ',', '(', ')', '.', ',', ':', '(', ')', '(', ')', '[', '].', ',', '.', ',', '.', ',', \"'\", \"'\", '.', ',', '-', '-', '.', ',', ',', ',', '.', ',', '.', ',', ',', ',', '.', '.', '-', '-', '-', '-', '[', '].', ',', '-', '-', '.', ',', ',', ':', '(', ')', \"'\", \"',\", ',', \"'\", \"'\", \"'\", \"'.\", ',', ',', ',', '(', ',', '),', ',', '.', ',', \"'\", '.', '(', '),', ',', ':', \"'\", \"'\", \"'\", \"'\", '(', ',', ').', ',', '(', '),', ',', '.', \"'\", \"'\", '.', ',', \"'\", \"'\", ':', '-', '(', '),', ',', '.', '-', '-', '-', ',', ',', '.', ',', ',', ',', '.', ',', ':', ',', ',', ',', '.', ',', \"'\", ',', '.', ',', '.', ',', '.', '-', '.', ',', \"'\", \"',\", ',', ',', '.', \"'\", '_', \"'\", ',', ',', ',', '.', ',', '-', '.', \"'\", \"'\", '.', ',', \"'\", '_', \"'\", \"'\", \"'\", '.', ',', '-', ',', ',', '-', '.', ',', ',', ',', '.', '-', '-', ':', '(', \"'\", '_', \"')\", '(', \"'\", '_', \"').\", ',', ',', ',', ',', ',', '.', '-', '.', '(', '.', '),', '-', \"('\", '_', \"')\", '-', '-', '.', \"('\", '_', \"')\", ',', '(', '),', ',', '.', ',', \"('\", '_', \"')\", \"'\", '.', ',', '.', \"'\", '_', \"'\", ',', ',', ',', ',', '-', '-', ',', '(', ',', '),', '.', ',', \"'\", '_', \"',\", '-', '-', ',', '.', ',', ',', \"'\", '_', \"'\", ',', '.', ',', ',', '.', ',', ',', '(', ',', ').', ',', '.', ',', ',', ',', '-', '.', '.', '.', '.', ',', '.', ',', '.', '.', ',', ',', \"'\", '.', '.', \"'\", \"'\", ',', '.', ',', ',', ',', '.', ',', ':', ',', ',', ',', ',', '.', ',', '.', ',', ',', ',', ',', ',', ',', '-', '.', '.', '(', ')', ',', '.', '.', ':', \"'\", '.', ',', '.', ',', '(', ')', '-', '.', ',', '-', ',', \"'\", \"'\", '.', '.', '-', ',', \"'\", '.', \"'\", ',', '.', ',', ',', ',', '.', ',', ',', ',', ',', ',', '.', ',', ',', '.', '-', ',', ',', '.', ',', '[', '].', ',', \"'\", '.', ',', '(', ').', ',', '.', \"'\", \"'\", '-', ',', '(', ').', '.', '-', ':', ',', '.', '.', ',', '.', ',', ',', \"'\", \"',\", '.', \"'\", \"'\", \"'\", \"'\", ',', \"'\", '.', ',', '.', \"'\", \"'\", '(', \"'\", '),', '(', '.', '.', '),', ',', ',', '.', ',', '.', ',', \"'\", \"'\", '.', ',', '.', ',', ',', ',', ',', ',', '.', '.', '-', ',', ',', ',', '.', '(', ',', '.', '),', ',', '.', '[', ']', '.', ',', ':', \"('\", \"',\", \"'\", \"',\", \"'\", \"'),\", ',', \"'\", \"'\", '.', ',', ',', '.', ',', '.', ',', '.', '(', ')', ',', \"'\", \"'\", \"'\", '.', ',', ',', '-', ',', ',', '.', ',', ',', '.', ',', \"'\", '.', ',', ',', \"'\", \"'\", '.', '.', ',', \"'\", '(', ').', ',', '.', '(', ')', '.', '.', ',', ',', \"'\", \"'\", '[', '],', '.', '(', ')', ',', '.', ',', '-', ',', \"'\", \"'\", ',', '(', '[', '],', ').', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', '.', '-', ',', '-', '-', ',', ',', '.', ',', '-', '-', ',', ',', ',', ',', '-', '.', '(', ')', '[', ']', ':', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', '-', '-', '-', '.', ',', '.', '[', ']', ':', '.', '.', '-', '-', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', '-', '-', '-', '-', '.', ',', '.', '[', ']', ':', '.', '.', '-', ',', '.', '[', ']', ':', '.', '.', '-', '-', '-', '-', '-', '-', '.', ',', '.', '(', ')', ':', '.', '.', ',', '.', ':', '.', ',', '.', ':', '.', '.', ',', '.', ':', '.', '.', '-', ',', '.', ':', '(', '.', '.)', ':', '-', '-', '-', ':', '-', '.', ',', '.', '.', '.', ',', ',', ',', '.', ',', '.', '-', ':', ':', ':', '.', '.', '.', '-', '.', '.', '.', '.', '.', '.', '.', '-', '(', ').', ',', '.', ',', ',', '-', ',', ',', '.', ',', ',', ',', ',', '.', '-', '[', '].', '(', ')', '(', ':', ').', '(', '.', ')', '[', '].', '.', ',', ':', '(', ')', ',', '(', ':', ').', '(', '.:', ').', ',', '(', '-', '),', '[', '].', '-', '(', ')', '(', ':', '-', '-', ';', '.', ').', ',', ',', ',', '[', '].', '.', '[', ']:', ',', ',', '(', '.', '.', ')', '.', ',', ',', '-', '.', ',', '.', ',', ',', '[', '].', ',', '-', '.', '(', ':', ')', '-', '(', ',', '),', '-', '(', '),', '-', '(', ')', '.', '-', '-', ',', '-', '.', '(', ')', ',', '-', '(', '.', '.', ',', ')', ',', '-', ',', '(', ').', '.', ',', ',', '-', '-', '[', '].', ',', ',', '[', '].', ',', ',', '-', ',', '-', '.', '[', ']:', ',', \"'\", \"',\", ',', ',', '.', '[', ']:', ',', \"'\", \"'\", '.', ',', ',', '(', ')', '(', '.).', \"'\", \"'\", \"'\", ',', '(', '.;', '.', ').', ',', ',', '-', '[', '].', '(', '.', ';', ')', '-', '(', ')', '(', ').', '(', ')', ',', '-', ',', '-', '.', '.', '(', ':', '),', '.', ',', ',', '-', '[', '].', ',', '(', ')', '[', '],', ',', ',', ',', ',', '[', '].', \"'\", \"',\", '(', '.).', ',', '(', '.', '.', ')', '(', '.).', '(', '.):', ',', '.', '.', ',', '(', ').', ',', '(', '.', ').', ',', '[', '].', ',', ',', ',', '(', '.', ').', '(', '.', ')', '(', '):', ',', '(', ',', '.)', '.', '.', ',', '-', '.', ',', '(', '.', ')', '[', '],', ',', '[', '].', ',', ',', '-', '(', ':', '-', ').', '(', '.).', ',', '[', '].', ',', '(', ':', '-', ').', '.', ',', ',', '-', '-', '(', '.).', '(', ',', ',', \"'\", \"'-\", ';', '[', '];', '.', ').', ',', ',', '[', ']', '.', ',', '.', '(', ')', '-', '.', ',', '-', ',', ',', '-', '[', '].', ',', ',', ',', '(', '.', '.', ').', '(', ')', ',', '(', '),', '(', ').', ',', ':', ',', ',', '.', ',', '.', '(', ',', '.', '.', '[', '])', ',', '-', ',', '.', '.', ',', ',', ',', '.', ',', ':', \"'\", \"'\", '(', ')', ',', '[', '].', ',', ',', '(', ',', '[', '])', '.', '-', ',', '.', ',', '-', ',', '-', '[', '].', ',', '(', ')', ',', '-', '.', ',', ',', ',', '(', ')', '-', '(', ':', ').', ',', ',', '-', '.', ',', '-', '.', ',', ',', ',', '.', '.', '\"', ':', '.\"', ':', '.', '.', '.', '.', '.', '.', ':', ':', '.', '.', '-', '-', '-', '.', '.', ':', '.', ',', '.', '.', '\"', ':', '.\"', ':', '.', '(', ').', '.', '.', '.', '.', ':', ':', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '?', '.', ':', '.', ',', ',', ',', ',', '.', '.', ',', ',', ',', '.', '.', '\"', ',', '.\"', ':', '.', ',', '.', '-', '.', '.', ':', ':', '.', '-', '.', '.', ':', '.', ',', '.', ',', ',', '.', '.', '\"', '.\"', ':', '.', '-', '.', ',', '.', '.', ':', ':', '.', '-', '.', '.', ':', '.', ',', '.', '.', '.', ':', '.', ',', ',', '.', '.', '.', ':', '-', '.', ',', ',', ',', '.', '.', '\"', '.\"', ':', '.', ',', '.', '-', '.', '.', ':', ':', '.', '.', '_', '.', '.', ':', '.', ',', ',', '.', '.', '\"', ':', '.\"', ':', '-', '.', ',', '.', '.', '.', ':', ':', '.', '-', '.', '.', ':', '.', ',', ',', '.', '.', '\"', '.\"', ':', '.', ',', '.', '-', '.', '.', ':', ':', '.', '.', '-', '.', '.', ':', '.', ',', ',', '.', '.', '\"', '-', '-', '.\"', ':', '.', ',', '.', '-', '.', '.', ':', ':', '.', '.', '_', '_', '_', '_', '.', '.', ':', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', '.', '?', ',', '.', '[', ']', ':', '.', '.', '-', '-', '-', ',', '.', '[', ']', ':', '.', '.', '-', '-', '-', ',', '.', '[', ']', ':', '.', '.', '.', '?', '_', '_', ',', '.', '[', ']', ':', '.', '.', '-', '-', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', '.', '?', '_', ',', '.', '[', ']', ':', '.', '.', '.', '?', ',', '.', '.', ',', '.', '.', '(', ':', '-', '-', ')', '-', ':', '(', ',', ',', ')', ',', '...', ',', '...', ',', '...', ',', '...', '.', '(', ')', ',', '(', ')', '.', ',', ',', '(', ':', ').', '.', '(', ')', ',', '(', ';', ':', '-', '):', '.', '.', '...', '...', '.', '.', '(', '[', ']', '[', '])', ':', ',', ',', '.', '(', ').', '?', '?', '(', ',', '...', ',', '.)', ',', '(', '.', '.', '?', ',', ').', '?', ',', '.', ':', ':', ',', ',', '.', '(', '[', ']', '[', '])', ':', ':', ':', ':', ':', '\"', '\"', ':', '(', ').', ':', ':', '\"', '\"', '.', ':', ':', '\"', '\"', ':', ':', '\"', '\"', ':', '(', ').', ':', ':', '\"', '\"', ':', ':', '\"', '\"', ':', ':', '\"', '\"', ':', ',', ':', '(', ',', '.).', ':', ':', '\"', '\"', ':', '\"', '\"', ':', '.', '.', ',', '(', ':', '-', ')', '.', ':', '?', ',', ':', ':', ':', '?', ',', ':', ':', '.', ':', '.', ':', '-', ',', '.', ':', ':', '-', '.', ':', '-', ':', '-', '-', ':', ':', ':', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', ':', ':', ':', ':', ':', ':', ':', \"'\", \"',\", ',', '.', '.', ',', ',', ',', '.', \"'\", \"'\", ',', ',', '.', '.', '(', ':', ').', ',', ',', ',', ',', ',', '(', ':', '-', ').', ',', '(', ')', '(', ';', '.', ').', ',', ',', '.', '(', ',', ')', ',', ',', ',', ',', ',', '(', ':', '-', ').', ',', ':', '-', ',', ',', ',', ',', ',', '-', '.', ',', '\"', '\"', ',', '.', ',', '\"', '\"', '.', '[...]', '(', ':', ')', '.', ',', '.', '(', ':', ')', ',', ',', '.', '(', ':', '-', ').', ',', '(', ':', '),', '.', '.', ',', '.', '(', ':', ')', ',', '(', ':', '-', ')', ',', ',', ',', ',', '.', ',', ',', ',', ',', \"'\", \"'\", '.', '(', '),', '.', ',', ',', '.', ',', '.', '.', \"'\", \"',\", ',', '(', '.', '.', ':', ',', ':', '),', '.', ',', \"'\", \"'\", '[', '].', \"'\", \"'\", '(', ':', '-', '),', ',', '(', ',', ').', ',', \"'\", \"',\", \"'\", \"',\", '(', ':', ').', '(', ':', '-', ')', \"'\", \"'\", \"'\", \"'\", \"'\", \"':\", ',', ',', \"'\", \"'\", ',', '.', '(', ':', '),', ',', ',', '-', ',', '(', ':', ').', '?', ',', ',', ',', ',', '(', ':', ').', ',', '(', ':', '),', ',', ',', '.', ',', ',', ',', ',', '(', ':', '-', ').', '.', ',', '(', '.', '.', ',', '),', '(', ':', ').', '(', ',', ',', ':', '-', ',', ':', ').', ',', ',', '.', ',', ',', ',', '.', ',', '(', ':', ')', '(', ':', ')', ',', ',', '.', ',', ',', ',', '(', ':', ').', ',', ',', ',', '(', ',', ',', ',', '.)', ',', ',', '(', ':', ').', ',', ',', '.', ',', '(', ':', ')', '[', '],', '.', ',', ',', ',', '(', ').', ',', '(', ':', ')', '.', '.', ',', '.', '(', ',', ',', ':', ',', ':', ')', ',', '(', ':', ').', ',', ',', '(', ':', ').', ',', ',', ',', ',', '(', ':', ',', ':', '-', ').', ',', ',', '(', ':', ')', ':', '.', ',', '.', ',', '(', ':', ',', ':', ').', '(', ':', ')', '(', ':', ').', ',', ',', '(', ':', '),', ',', '(', ':', ').', ',', '-', \"'\", \"'\", '.', '.', ',', ',', ',', ',', ':', '.', ':', ',', '(', ':', '-', '),', ',', '.', '(', ':', ')', ',', '[', '],', '[', '],', '.', ',', '(', ')', ',', ',', '.', ',', '.', ',', \"'\", \"'\", ',', '(', '):', ':', '(', ')', ',', '[', '].', ':', ',', '.', ':', ',', ',', '(', ')', ',', ',', ',', ',', ',', ',', '.', ',', '(', ',', ',', ':', ',', ').', ',', '(', '-', '-', ')', '[', '-', ']', '.', '(', ';', ')', '(', '),', '.', ',', ',', '.', ',', '.', '[', ']', '-', '[', '].', ',', '[', '],', '.', '[', ']', '(', '(', ':', ')', '),', '.', ',', ',', '(', ':', '),', ',', ',', '.', ',', ',', '.', '.', ':', ',', ',', ',', '(', '-', ':', ').', '.', ',', ',', ',', '.', ',', '(', ':', ')', ',', ',', ',', '-', ',', '.', ',', ',', ',', ',', '.', '-', '-', ',', '-', '-', '-', ',', '(', '.', '),', ':', '-', '-', ',', ',', ',', ',', '.,', '.', ',', '[', ']', ',', ',', ',', '.', '.', '[', '],', ',', '.', '-', '(', '.', '),', ',', ',', ',', ',', '.,', '.', '.', ',', '[', ']', ',', ',', ',', '(', ',', '[', ']).', ',', ',', '.', ',', \"'\", \"'\", ',', ':', ':', '?', ':', '?', ',', '?', ':', '?', '(', ')', '?', '?', ':', ',', '?', '.', '.', '.', ':', ',', ',', '(', '.', '),', ',', '(', '.', '),', ',', ',', \"'\", \"'\", ',', ',', '(', '.', ').', ',', '(', ')', ',', '.', ',', ',', '-,', '[', '].', '.', '.', ',', ':', '(', ':', ')', '(', ')', '.', ',', ':', '(', '),', ',', '.', ',', '(', '.', ')', ',', ',', ',', '(', '.', ')', ',', ',', '(', '.', ')', '.', ',', ',', ',', '.', ',', ':', ',', ',', ',', '?', '(', ')', '.', ',', ',', '.,', '.', '-.', ',', ',', '-', '-', '-', '(', '),', ',', ',', ',', ',', '.', ',', '.', ',', '.', ',', '-', '-', ',', '(', '.', '.', ',', ').', ',', ',', ',', '.', '(', '.', '.', '),', '.', '(', '),', '.', ',', '.', '.', ',', '.', ',', '.', '.', '.', ',', '.', ',', ',', ',', ',', '(', ',', ').', '(', ')', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', ',', '(', ':', ').', ',', '.', ':', '.', '(', ')', ',', \"'\", \"'\", \"'\", \"':\", \"'\", '[', ']', '.', '[', ']', '.', '.', ':', '-', '.', '-', '.', '-', \"'\", '[', '].', '.', ',', ':', '[', '].', ',', ',', ',', '.', ',', '(', ',', ',', ',', ').', ',', '(\"', \"'\", '\",', ':', ')', '.', ',', ',', '-', ',', \"'\", \"'\", \"'\", \"'.\", '.', ',', '.', '(', ')', '.', ',', ',', '.', '(', '.', '.', ')', '.', ',', '.', '.', '.', \"'\", \"'\", ',', '.', ',', ',', ',', '(', ',', ',', ',', '.)', '.', ',', ',', '.', '(', ').', ',', ',', ',', ',', '.', ',', ',', '(', ')', '.', '(', '.', '.', ',', ',', ',', ',', '.).', ',', '.', ',', '(', ',', ')', '(', ')', '.', ',', '-', ',', '.', '[', ']', '.', ',', '.', ',', ',', ',', ',', '.', ',', '-', ',', '.', ',', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', '.', \"'\", \"'\", ':', '-', ',', '.', ',', '(', ',', ')', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', ',', '.', '.', ',', '(', '.', '.', '),', ',', '.', ',', ',', '.', ',', ',', '.', ',', '.', '.', '\"', ':', '.\"', ',', ',', '.', '(', '.):', \"'\", ':', '.', ':', '.', '.', '.', ',', '.', '.', '\"', '.\"', ':', '.', '(', ').', '-', '.', '.', '.', '.', ':', ':', '.', '.', '.', ':', '.', ',', ',', ',', ',', ',', '.', '.', '.', ':', '.', '.', ',', '.', '.', ':', ':', '.', '.', '.', '.', ':', '.', ',', '.', '.', ':', \"'\", \"'.\", ':', '.', ',', '.', '.', '.', '\"', \"'\", \"'\", ':', '.\"', ':', '.', '(', ').', '.', '.', '.', '.', ':', ':', '.', '.', '?', '_', '.', '.', '.', ':', '.', ',', '-', ',', '.', '.', '\"', '?', '.\"', ':', '.', '(', ')', '.', '.', '.', '.', ':', ':', '.', '.', '.', ':', '.', ',', '.', ',', '.', '.', ':', '.', ',', ':', '.', ',', '.', '.', '\"', '.\"', ',', '.', ',', ',', ',', '.', '(', '.):', ':', '.', ':', '-', '.', '.', '.', ',', '.', '.', ':', '.', ':', '.', ',', '.', '.', ':', '.', ':', '.', '[', ']', ':', '.', '.', '.', '.', '.', '-', ',', '.', '[', ']', ':', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '[', ']', ':', '.', '-', '.', ',', '.', '[', ']', ':', '.', '.', '.', '.', '.', '-', ',', '.', '[', ']', ':', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', '.', ',', '.', '[', ']', ':', '.', '.', ',', '.', '.', ':', ',', '(', '):', '(', ')', ':', ',', '\"', '\"', '[', ']', '\"', '\"', '\"', '\"', '.', ',', ',', '\"', '\"', '-', '\"', '\"', ',', '.', '?', '.', ',', ',', '(', ':', ')', '____________________________________________________________________', '...', ':', '.', '\"', '\".', '\"', '\"', '.', '\"', '\".', '\"', '\"', '(', ',', ')', '.', '\"', '\"', ':', '\"', '-', '\",', '\"', '-', '\",', '\"', '-', '\"', '\"', '-', '\".', ',', '.', '.', ',', ',', '(', ':', ')', '____________________________________________________________________', ':', '-', ',', ',', ',', ',', ',', '-', '.', ',', '\"', '\"', ',', '.', ',', '\"', '\"', '.', '\"', '\"', ',', ';', ',', '\"', '.\"', ',', '\"', '\"', ',', ',', ',', ',', '-', ',', ',', '\"', \"'\", '.\"', '\"', '\"', ',', ',', '.', ',', \"'\", '.', ',', ',', ',', '(', ':', ')', ':', '(', ')', ':', '-', '-', '(', ')', ':', ',', ',', \"'\", ',', ',', '-', ':', ':', ',', '(', '[', '])', '____________________________________________________________________', '-', '-', ',', ',', '[', ']:', '-', '.', '(', '[', '])', '____________________________________________________________________', '-', '-', ',', '-', ':', ',', ',', '-:', ':', ':', '-', ':', ':', '-', '-:', '-', '-', '-:', ',', ',', ',', '(', '-', '[', '])', '____________________________________________________________________', '-', '((', '-', ')', ')', ':', \"'\", '-', '.', ':', '-', '.', \"'\", \"',\", '.', '(', '[', '])', ':', ':', '(', ',', ')', '(', '.', '.', ')', '(', '.', ')', '(', '.', '.', ')', '(', '.', '.', ')', '(', '.', '.', ')', '-', '(', ',', ')', '(', '.', '.', ')', '(', '.', '.', ')', '(', '.', ')', '(', '.', ')', '(', '.', ')', '(', ',', ')', '(', '.', '.', ')', '(', '.', '.', ')', '(', ')', '(', '.', '.', ')', '(', '.', ')', ':', '.', ',', '.', '.', ',', ',', '[', '].', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', '(', ')', ':', '(', ',', ')', '(', '.', ')', '(', '.', '.', ')', '(', '.', '.', ')', '(', '.', '.', ')', '(', '.', '.', ')', ',', '.', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', '),', '(', ')', ':', '-', '-', '[', ']:', '-', '.', ';', ',', ':', ':', ',', '.', ',', ':', ',', '.', ',', ',', ',', ',', ':', ',', '-', ',', ',', '-', '.', ',', ',', ':', ',', '.', ',', ':', ',', '.', ',', ':', ',', '-', '.', '(', '.', '.', ')', ':', ',', '-', '____________________________________________________________________', '-', '.', ',', ':', ',', '.', ':', '.', ',', '[', ']', ':', ',', ',', ',', '.', ',', '.', '.', ',', ',', ',', '.', '(', ',', ',', '.)', '.', '.', ',']\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "pattern = r'''(?x)\n",
    "    [\\.,;\"'?\\(\\):\\-_`\\[\\]\\{\\}]+ # one or more punctuation symbols, brackets etc.\n",
    "'''\n",
    "print nltk.regexp_tokenize(corpusText, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.50$', '8 $', '9$', '$2.40', '2016-11-01', '2 March 1998', '19 January 2001', 'Sam', 'United Nations']\n"
     ]
    }
   ],
   "source": [
    "# b\n",
    "pattern = r'''(?x)\n",
    "    (?:\\d+\\.)?\\d+\\s?\\$                  # Monetary amount like 2.40$\n",
    "    | \\$\\s?(?:\\d+\\.)?\\d+                # Monetary amount like $2.40\n",
    "    | \\d{4}\\-\\d{2}\\-\\d{2}               # Date like 2016-22-01\n",
    "    | \\d{1,2}\\s[A-Z][a-z]{2,8}\\s\\d{4}   # Date like 2 March 1998\n",
    "    | [A-Z][a-z]+(?:\\s[A-Z][a-z]+)?     # Proper Names - TODO: don't match beginning of sentence\n",
    "'''\n",
    "testString = 'should match 3.50$ or 8 $ or 9$ or $2.40 or 2016-11-01 or 2 March 1998 or 19 January 2001 or Sam or United Nations'\n",
    "print nltk.regexp_tokenize(testString, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Web', 'Based', 'Assessment Beyond', 'Multiple', 'Choice', 'The Application', 'Technologies', 'Different Testing', 'Formats', 'Documentation', 'Master', 'Thesis', 'Linguistics', 'Web Technology', 'Faculty', 'Foreign Languages', 'Cultures', 'Philipps', 'Universit', 'Marburg', 'Julia Neumann', 'Naumburg', 'Germany', 'Marburg', 'Contents', 'List', 'Abbreviations', 'Introduction', 'User Guide', 'Overall Organization', 'Code', 'General Design', 'Java', 'Script Components', 'Implementation', 'Testing Formats', 'Crossword', 'Dynamic Multiple', 'Choice', 'Drag', 'Drop', 'Database Structure', 'General Features', 'Index Page', 'Contact Page', 'Color Changer', 'Inline Editing', 'Deletion', 'Exporting Tests', 'References', 'Appendix', 'Database Structure', 'Declaration', 'Authorship', 'List', 'Abbreviations', 'Asynchronous Java', 'Script', 'Cascading Style', 'Sheets', 'Document Object', 'Model', 'Hypertext Markup', 'Language', 'Joint Photographic', 'Experts Group', 'Model', 'View', 'Controller\\nMy', 'Li', 'My', 'Improved', 'Hypertext Preprocessor', 'Portable Network', 'Graphics', 'Structured Query', 'Language', 'Scalable Vector', 'Graphics', 'Extensible Markup', 'Language', 'Introduction\\nThis', 'The', 'Thus', 'These', 'Users', 'Modern', 'While', 'Java', 'Script', 'The', 'Moreover', 'Before', 'User Guide', 'The', 'This', 'Hovering', 'Apart', 'They', 'The', 'By', 'Clicking', 'To', 'Therefore', 'Instead', 'Home', 'Clicking', 'The', 'Short', 'In', 'Elements', 'This', 'Pressing', 'Enter', 'Elements', 'Clicking', 'When', 'Furthermore', 'Running', 'The', 'Check', 'While', 'Overall Organization', 'Code\\nThe', 'Java', 'Script', 'The', 'Furthermore', 'On', 'Its', 'My', 'Li', 'Objects', 'Furthermore', 'Lastly', 'It', 'Another', 'These', 'The', 'Java', 'Script', 'Java', 'Script', 'Aside', 'For', 'These', 'Java', 'Script', 'The', 'The', 'Java', 'Script', 'These', 'Furthermore', 'This', 'Java', 'Script', 'Finally', 'Apart', 'This', 'Java', 'Script', 'It', 'Java', 'Script', 'As', 'Java', 'Script', 'Java', 'Script', 'Therefore', 'Java', 'Script', 'General Design', 'Java', 'Script Components', 'As', 'Java', 'Script', 'Model', 'View', 'Controller', 'The', 'Combining', 'Java', 'Script', 'The', 'Java', 'Script', 'Test', 'Test', 'Item', 'View', 'Control', 'These', 'View', 'In', 'Java', 'Script', 'Test', 'View', 'Control', 'The', 'Test', 'Item', 'Question', 'Item', 'Container', 'These', 'While', 'View', 'When', 'The', 'Whenever', 'Implementation', 'Testing Formats', 'While', 'Java', 'Script', 'Crossword\\nFor', 'Answers', 'English', 'Once', 'The', 'Java', 'Script', 'Thus', 'The', 'Given', 'Then', 'For', 'If', 'Once', 'This', 'If', 'The', 'Thus', 'First', 'This', 'Second', 'Should', 'The', 'As', 'Web Worker', 'This', 'Before', 'Web Workers', 'In', 'Once', 'Java', 'Script', 'When', 'Apart', 'Simply', 'An', 'Upon', 'Dynamic Multiple', 'Choice\\nWhile', 'As', 'Thus', 'This', 'As', 'By', 'It', 'For', 'In', 'As', 'For', 'Here', 'Simple', 'Apart', 'As', 'Java', 'Script', 'This', 'Lastly', 'Thus', 'When', 'This', 'Drag', 'Drop\\nThe', 'While', 'The', 'The', 'Thus', 'Once', 'Elements', 'Dragenter', 'When', 'The', 'Finally', 'The', 'In', 'This', 'Another', 'Learners', 'In', 'Java', 'Script', 'Database Structure', 'Complementing', 'My', 'For', 'Appendix', 'The', 'On', 'The', 'This', 'The', 'Ds', 'Similarly', 'Ds', 'This', 'On', 'The', 'Each', 'Item', 'The', 'As', 'The', 'Thus', 'Lastly', 'The', 'This', 'Strictly', 'Storing', 'It', 'Therefore', 'General Features', 'Apart', 'The', 'Index Page', 'The', 'This', 'The', 'More', 'Assuming', 'The', 'The', 'As', 'Therefore', 'First', 'Second', 'Java', 'Script', 'This', 'The', 'Contact Page', 'There', 'The', 'Apart', 'Internet', 'As', 'Color Changer', 'In', 'This', 'This', 'Changing', 'Java', 'Script', 'In', 'As', 'This', 'Local Storage', 'Each', 'Whenever', 'If', 'This', 'Inline Editing', 'Deletion\\nAnother', 'While', 'Displaying', 'Therefore', 'For', 'The', 'View', 'Thus', 'When', 'Also', 'Following', 'These', 'As', 'Exporting Tests', 'Administrators', 'As', 'The', 'Basically', 'When', 'Download', 'Download', 'Print Test', 'Object', 'This', 'Next', 'Blob', 'This Blob', 'As', 'Object', 'Blob', 'The', 'If', 'Once', 'If', 'Data', 'This', 'In', 'The', 'It', 'First', 'Internet Explorer', 'Object', 'The', 'Second', 'Data', 'Blob', 'Chrome', 'Opera', 'Thus', 'Firefox', 'Printing', 'Chrome', 'Opera', 'Firefox', 'Again', 'As', 'This', 'Local Storage', 'Web Workers', 'Web', 'References\\nInternet', 'Sources', '06 May 2015', '06 May 2015', '06 May 2015', '11 May 2015', '07 May 2015', '07 May 2015', '11 May 2015', 'Web', 'Element', 'Object', '12 May 2015', '11 May 2015', 'Further Internet', 'Sources', '07 May 2015', '07 May 2015', '07 May 2015', '07 May 2015', 'Appendix', 'Database Structure', 'Declaration', 'Authorship', 'Master', 'Thesis', 'Master', 'Arts', 'Web', 'Based', 'Assessment Beyond', 'Multiple', 'Choice', 'The Application', 'Technologies', 'Different Testing', 'Formats', 'No', 'All', 'The', 'Internet', 'Marburg', '19 May 2015', 'Markup Languages', 'Human Language', 'Technologies', 'Three Examples', 'Julia Neumann', 'Paper', 'Human Language', 'Technologies', 'Winter Term', 'Submission Date', '17 December 2013', 'Approved', 'Dr', 'Peter Franke', 'Philipps University', 'Marburg\\nContents', 'List', 'Abbreviations', 'Introduction', 'Overview', 'Markup Language', 'Advantages', 'Applications', 'Languages', 'Conclusions', 'References', 'Appendix', 'Example', 'Example', 'Example', 'Example', 'List', 'Abbreviations', 'Artificial Intelligence', 'Markup Language', 'Document Type', 'Definition', 'Human Language', 'Technologies', 'Hypertext Markup', 'Language', 'Natural Language', 'Processing', 'Web Ontology', 'Language', 'Standard Generalized', 'Markup Language', 'World Wide', 'Web Consortium', 'Extensible Markup', 'Language', 'Path', 'Path Language', 'Extensible Stylesheet', 'Language Transformations', 'Introduction', 'Overview', 'The', 'In', 'Furthermore', 'But', 'Extensible Markup', 'Language', 'The', 'Lobin', 'Its', 'Markup Language', 'In', 'Markup', 'Lobin', 'However', 'This', 'Lobin', 'More', 'Advantages', 'Applications', 'The', 'Firstly', 'Secondly', 'Moreover', 'These', 'As', 'Schwartzbach', 'Languages', 'Considering', 'Artificial Intelligence', 'Markup Language', 'Web Ontology', 'Language', 'Extensible Stylesheet', 'Transformations', 'Our', 'It', 'The', 'Each', 'These', 'For', 'Furthermore', 'Other', 'These', 'Contemporary', 'Fialho', 'Silvervarg', 'As Bii', 'While', 'Silvervarg', 'It', 'Semantic Web', 'Web', 'Such', 'In', 'The', 'The', 'Class', 'By', 'To', 'Web', 'In', 'Androutsopoulos', 'Another', 'Sateli', 'Semantic Assistants', 'Witte', 'Gitzinger', 'Here', 'After', 'Being', 'Lobin', 'These', 'In', 'More', 'Lobin', 'The', 'This', 'Path', 'For', 'Gill', 'Similarly', 'Of', 'Sch', 'Weitz', 'Conclusions\\nThe', 'Of', 'Using', 'As', 'Still', 'Other', 'Other', 'Furthermore', 'However', 'Moreover', 'Path', 'Still', 'Lobin', 'Although', 'Due', 'References\\nAndroutsopoulos', 'Ion', 'Lampouras', 'Gerasimos', 'Galanis', 'Dimitros', 'Generating Natural', 'Language Descriptions', 'Ontologies', 'Natural', 'System', 'In', 'Journal', 'Artificial Intelligence', 'Research', 'November', 'Available Online', 'Last', '30 November 2013', 'Bii', 'Patrick', 'Chatbot Technology', 'Possible Means', 'Unlocking Student', 'Potential', 'Learn How', 'Learn', 'In', 'Educational Research', 'Februar', 'Available Online', 'Last', '30 November 2013', 'Fialho', 'Pedro', 'Coheur', 'Lu', 'Curto', 'Costa', 'Pedro', 'Abad', 'Alberto', 'Meinedo', 'Hugo', 'Trancoso', 'Isabel', 'Meet', 'In', 'Proceedings', 'Annual Meeting', 'Association', 'Computational Linguistics', 'Sofia', 'Bulgaria', '9 August 2013', 'Available Online', 'Last', '28 November 2013', 'Gill', 'Alastair', 'Brockmann', 'Carsten', 'Oberlander', 'Jon', 'Perceptions', 'Alignment', 'Personality', 'Generated Dialogue', 'In', 'Proceedings', 'International Natural', 'Language Generation', 'Conference', 'May', '1 June 2012', 'Utica', 'Available Online', 'Last', '29 November 2013', 'Lobin', 'Henning', 'Computerlinguistik', 'Texttechnologie', 'Paderborn', 'Fink', 'Anders', 'Schwartzbach', 'Michael', 'An Introduction', 'Web Technologies', 'Harlow', 'Addison', 'Wesley', 'Sateli', 'Bahar', 'Cook', 'Gina', 'Witte', 'Ren', 'Smarter Mobile', 'Apps', 'Integrated Natural', 'Language Processing', 'Services', 'In', 'Proceedings', 'International Conference', 'Mobile Web', 'Information Systems', 'Paphos', 'Cyprus', '28 August 2013', 'Available Online', 'Last', '30 November 2013', 'Sch', 'Ulrich', 'Weitz', 'Benjamin', 'Combining', 'Outputs', 'Logical Document', 'Structure Markup', 'Technical Background', 'Contributed Task', 'In', 'Proceedings', 'Special Workshop', 'Rediscovering', 'Years', 'Discoveries', 'Jeju', 'Republic', 'Korea', '10 July 2012', 'Available Online', 'Last', '30 November 2013', 'Silvervarg', 'Annika', 'Arne', 'Iterative Development', 'Evaluation', 'Social Conversational', 'Agent', 'In', 'Proceedings', 'International Joint', 'Conference', 'Natural Language', 'Processing', 'Nagoya', 'Japan', '18 October 2013', 'Available Online', 'Last', '28 November 2013', 'Witte', 'Ren', 'Gitzinger', 'Thomes', 'Semantic Assistants', 'User', 'Centric Natural', 'Language Processing', 'Services', 'Desktop Clients', 'In', 'Proceedings', 'Asian Semantic', 'Web Conference', 'Bangkok', 'Thailand', '11 December 2008', 'Available', 'Online', 'Last', '30 November 2013', 'Internet Sources', '21 November 2013', '23 November 2013', '28 November 2013', '28 November 2013', '28 November 2013', '30 November 2013', '30 November 2013', 'Web', 'Ontology', 'Language', '30 November 2013', '29 November 2013', '29 November 2013', '30 November 2013', 'Procedural', '6 December 2013', '7 December 2013', 'Appendix\\nThis', 'It', 'Example\\nThe', 'Lobin', 'In', 'Within', 'Lobin', 'The', 'Lobin', 'Example', 'This', 'When', 'This', 'Do', 'Star Wars', 'No', 'The', 'Which', 'Why', 'Star Wars', 'The', 'The', 'It', 'Example\\nThe', 'Class', 'Class', 'Of', 'Class', 'Thing', 'There', 'Thing', 'Thing', 'The', 'Object', 'Property', 'Color', 'Object', 'Property', 'Here', 'It', 'Color', 'Thing', 'Color', 'Thing', 'The', 'Example\\nIn', 'Lobin', 'The', 'Hi', 'How', 'In', 'User', 'Chatbot', 'Hi', 'How', 'For', 'This', 'User', 'Parts', 'The', 'Path', 'Chatbot', 'The', 'An', 'Uber', 'Exceptional', 'Perspective', 'Borrowing', 'Corpus', 'Based Case', 'Study', 'German Loan', 'Morpheme', 'Present', 'Day English', 'Julia Neumann', 'Paper', 'The New', 'Media', 'Linguistics', 'Corpus Linguistics', 'Summer Term', 'Submission Date', '19 September 2014', 'Approved', 'Prof', 'Dr', 'Rolf Kreyer', 'Philipps University', 'Marburg\\nContents', 'Abstract', 'Introduction', 'Theoretical', 'Empirical Background', 'General Aspects', 'Terminology', 'Adaptation', 'Loans', 'German Loans', 'English', 'Corpus Study', 'Example', 'German Loan', 'Morpheme', 'Choice', 'Corpus', 'Corpus Research', 'Orthography', 'Grammar', 'Semantics', 'Usage', 'Comparison', 'Donor Language', 'Conclusions', 'References', 'Appendix', 'Speaker Intuitions', 'Appendix', 'English Dictionary', 'Information', 'Appendix', 'Research Results', 'Table', 'Different Word', 'Classes', 'English', 'Table', 'Words Used', 'Most Commonly', 'English', 'Table', 'Different Word', 'Classes', 'German', 'List', 'Items Used', 'Most Commonly', 'German', 'Appendix', 'German Dictionary', 'Information', 'Confirmation', 'Authorship', 'Abstract\\nLoan', 'The', 'For', 'German', 'English', 'Ten', 'Ten', 'The', 'German', 'Introduction\\nOne', 'Haugen', 'While', 'English', 'German', 'German', 'English', 'Stanforth', 'However', 'German', 'German', 'English', 'Limbach', 'German', 'One', 'The', 'English', 'American', 'British English', 'English', 'Limbach', 'Thus', 'The', 'American', 'Recently', 'Relieved', 'American English', 'Limbach', 'One', 'In', 'As Stanforth', 'English', 'German', 'His', 'German', 'English', 'Stanforth', 'Accordingly', 'German', 'Pfeffer', 'Cannon', 'German', 'English', 'It', 'German', 'English', 'Such', 'Meier', 'Stubbs', 'German', 'Angst', 'English', 'German', 'Assuming', 'English', 'Corpus', 'German', 'However', 'Theoretical', 'Empirical Background', 'The', 'General Aspects', 'Terminology\\nWhile', 'Haspelmath', 'Haugen', 'In', 'Concise Oxford', 'Dictionary', 'Linguistics', 'Although', 'Haugen', 'Furthermore', 'Haspelmath', 'Haugen', 'While', 'All', 'Haugen', 'Haspelmath', 'Why', 'There', 'Haspelmath', 'Thus', 'Meier', 'In', 'Stanforth', 'Adaptation', 'Loans\\nNo', 'Haspelmath', 'Several', 'Pfeffer', 'Cannon', 'Haugen', 'However', 'First', 'For German', 'English', 'Meier', 'Pfeffer', 'Cannon', 'English', 'With', 'Haugen', 'It', 'German', 'English', 'English', 'German', 'Stanforth', 'As', 'With', 'Stubbs', 'German', 'English', 'German', 'One', 'Stanforth', 'In', 'Pfeffer', 'Cannon', 'German Loans', 'English\\nWhile', 'German', 'English', 'German', 'English', 'It', 'Stubbs', 'Stanforth', 'English', 'Stubbs', 'Possible Reasons', 'English', 'Great Britain', 'United States', 'Stanforth', 'Thus', 'English', 'German', 'Stubbs', 'Pfeffer', 'Cannon', 'The', 'German', 'Stubbs', 'There', 'English', 'The', 'The', 'German', 'German', 'Stubbs', 'Stanforth', 'Estimates', 'German', 'English', 'Stanforth', 'Stubbs', 'In', 'German', 'English', 'Stanforth', 'English', 'Pfeffer', 'Cannon', 'Corpus Study', 'Based', 'German', 'Example', 'German Loan', 'Morpheme\\nAs', 'German', 'English', 'German', 'This', 'First', 'Limbach', 'English', 'The', 'Stanforth', 'English', 'German', 'British National', 'Corpus', 'Second', 'Limbach', 'Furthermore', 'The', 'English', 'Appendix', 'The', 'The', 'English', 'Apart', 'Stubbs', 'Meier', 'For', 'English', 'All', 'Appendix', 'With', 'Additionally', 'Longman Dictionary', 'Contemporary English', 'Merriam', 'Webster Dictionary', 'Further', 'While', 'Oxford English', 'Dictionary', 'German', 'Limbach', 'German', 'Dictionary', 'Meier', 'An', 'Choice', 'Corpus\\nThere', 'First', 'Onysko', 'Winter', 'Froemel', 'This', 'Second', 'English', 'Third', 'Stubbs', 'As', 'Ten', 'Ten', 'Web', 'This', 'Web', 'Jakub', 'It', 'Web', 'Furthermore', 'Sketch Engine', 'Ten', 'Ten', 'German', 'German', 'The', 'Ten', 'Ten', 'German', 'Another', 'Ten', 'Ten', 'Jakub', 'Corpus Research', 'For', 'Sketch Engine', 'Corpus Query', 'Language', 'As', 'Ten', 'Ten', 'German', 'The', 'English', 'Orthography', 'To', 'Grammar', 'Which', 'Is', 'Semantics', 'What', 'Which', 'What', 'Comparison', 'Which', 'German', 'The', 'Orthography\\nThe', 'German', 'German', 'German', 'These', 'German', 'English', 'Nothing', 'English', 'German', 'Grammar\\nAs', 'It', 'Stanforth', 'Thus', 'It', 'In', 'Consequently', 'Apart', 'Is', 'Table', 'Appendix', 'Taking', 'Thus', 'These', 'Also', 'Jennifer', 'There', 'Australia', 'What', 'Even', 'English', 'English', 'Second', 'This', 'Indeed', 'In', 'Rather', 'Semantics', 'Usage\\nAs', 'English', 'As', 'Sketch Engine', 'Python', 'Table', 'Appendix', 'Even', 'This', 'Limbach', 'As', 'This', 'Rafael', 'Christmas', 'Perennial', 'Duke', 'Mike Kzryzewksi', 'He', 'So', 'Or', 'The', 'Thus', 'No', 'Rumors', 'Rowling', 'Harry Potter', 'Limbach', 'Additionally', 'As', 'In Stanforth', 'German', 'English', 'Nazi Germany', 'Thus', 'German', 'Germany', 'Nazi', 'Hitler', 'German', 'Only', 'This', 'English', 'Comparison', 'Donor Language', 'To', 'German', 'In', 'Ten', 'Ten', 'Table', 'Appendix', 'The', 'German', 'These', 'Only', 'Accordingly', 'Appendix', 'Python', 'As', 'German', 'English', 'The', 'Duden Online', 'Appendix', 'Duden', 'However', 'English', 'Rather', 'In', 'German', 'English', 'German', 'Conclusions\\nThe', 'English', 'Its', 'German', 'Furthermore', 'German', 'These', 'English', 'Of', 'Diachronic', 'German', 'Investigating', 'Furthermore', 'In', 'It', 'English', 'References\\nHaspelmath', 'Martin', 'Lexical', 'Concepts', 'In Haspelmath', 'Martin', 'Tadmor', 'Uri', 'Loanwords', 'World', 'Languages', 'Comparative Handbook', 'Berlin', 'Walter', 'Gruyter', 'Haugen', 'Einar', 'The Analysis', 'Linguistic Borrowing', 'In', 'Language', 'April', 'June', 'Available Online', 'Last', '5 September 2014', 'Jakub', 'Milo', 'Kilgarriff', 'Adam', 'Kov', 'Vojt', 'Rychl', 'Pavel', 'Suchomel', 'The Ten', 'Ten Corpus', 'Family', 'In', 'Proceedings', 'International Corpus', 'Linguistics Conference', '26 July 2013', 'Lancaster', 'Available Online', 'Corpora', 'Ten', 'Ten', 'Last', '10 September 2014', 'Limbach', 'Jutta', 'Ausgewanderte', 'Eine Auswahl', 'Beitr', 'Ausschreibung', 'Ausgewanderte', 'Ismaning', 'Hueber', 'Meier', 'The Status', 'Foreign Words', 'English', 'The Case', 'Eight German', 'Words', 'In', 'American Speech', 'Summer', 'Available Online', 'Last', '5 September 2014', 'Onysko', 'Alexander', 'Winter', 'Froemel', 'Esme', 'Necessary', 'Exploring', 'In', 'Journal', 'Pragmatics', 'Available Online', 'Last', '5 September 2014', 'Pfeffer', 'Alan', 'Cannon', 'Garland', 'German Loanwords', 'English', 'An Historical', 'Dictionary', 'Cambridge', 'New York', 'Cambridge University', 'Press', 'Stanforth', 'Anthony', 'Functional', 'Stylistic Aspects', 'German Loans', 'English', 'In Flood', 'John', 'Salmon', 'Paul', 'Sayce', 'Oliver', 'Wells', 'Christopher', 'Das', 'Band', 'Sprache', 'Studies', 'German Language', 'Linguistic History', 'Memory', 'Leslie Seiffert', 'Stuttgart', 'Hans', 'Dieter Heinz', 'Akademischer Verlag', 'Stanforth', 'Anthony', 'Deutsche Einfl', 'Wortschatz', 'Geschichte', 'Gegenwart', 'Beitrag', 'Amerikanischen Englisch', 'Eichhoff', 'Niemeyer', 'Stubbs', 'Michael', 'Words', 'Phrases', 'Corpus Studies', 'Lexical Semantics', 'Oxford', 'Blackwell', 'Internet Sources', '9 September 2014', '10 September 2014', '10 September 2014', '8 September 2014', '8 September 2014', '8 September 2014', '8 September 2014', '10 September 2014', 'Corpora', 'Ten', 'Ten', '10 September 2014', 'Sk', 'Corpus', 'Querying', '15 September 2014', 'September', 'Appendix', 'Speaker Intuitions', 'Speaker', 'English', 'Limbach', 'Englisch', 'In', 'Jugendsprache', 'Steigerungsform', 'Alle', 'Klassen', 'Jugend Gro', 'Wort', 'Sprache', 'Umlaute', 'Nutzen', 'Wie', 'Great Britain', 'Das', 'Befragten', 'Christian Fuchs', 'Berlin', 'Deutschland', 'Limbach', 'Britisches Englisch', 'Beim Zeitunglesen', 'Grossbritannien', 'Woerter', 'Fuer', 'Gro', 'Meist', 'Hin', 'Wort', 'Uberflieger', 'Bedeutung', 'Weg', 'Presse', 'Vor', 'Zusammensetzung', 'Dadurch', 'Bedeutung', 'Wortes', 'Gegenteil', 'Je', 'Zusammenhang', 'Angelika Mohr', 'London', 'Gro', 'Limbach', 'Amerkanisches Englisch', 'The', 'American', 'Recently', 'Relieved', 'American English', 'The', 'That', 'Additionally', 'Rumors', 'Rowling', 'Harry Potter', 'The', 'American', 'American', 'Nazi', 'Aryan', 'This American', 'Nietzschean', 'Robert Keeley', 'Worcester', 'Massachusetts', 'Limbach', 'Appendix', 'English Dictionary', 'Information\\nEntries', 'English', 'Definitions', 'America', 'Bill Gates', 'Bohemians\\nWord', 'Origin', 'German', 'Collins English', 'Dictionary', 'Pamela Lee', 'Longman Dictionary', 'Contemporary English', 'Full Definition', 'Variants', 'Origin', 'German', 'Old High', 'German', 'Merriam', 'Webster Dictionary', 'German', 'Oxford Dictionary', 'English', 'Appendix', 'Research Results', 'Table', 'Different Word', 'Classes', 'English', 'Table', 'Words Used', 'Most Commonly', 'English\\nThe', 'Items', 'The', 'Connotations', 'Oxford Dictionary', 'English', 'Table', 'Different Word', 'Classes', 'German', 'List', 'Items Used', 'Most Commonly', 'German\\nThe', 'Appendix', 'German Dictionary', 'Information\\nEntry', 'Duden', 'Bildungen', 'Adjektiven', 'Verst', 'Gebrauch', 'Beispiel', 'Bildungen', 'Adjektiven', 'Eigenschaft', 'Beispiel', 'Bildungen', 'Adjektiven', 'Verben', 'Ma', 'Beispiele', 'Bildungen', 'Verben', 'Sache', 'Beispiel', 'Bildungen', 'Verben', 'Bedecken', 'Sicherstrecken', 'Beispiel', 'Bildungen', 'Substantiven', 'Endung', 'Sache', 'Oberseite', 'Beispiel', 'Bildungen', 'Verben', 'Wechseln', 'Stelle', 'Beispiel', 'Bildungen', 'Substantiven', 'Zuviel', 'Beispiel', 'Bildungen', 'Substantiven', 'Beispiel', 'Bildungen', 'Substantiven', 'Figur', 'Sache\\nBeispiel', 'Confirmation', 'Authorship', 'All', 'The', 'Internet', 'Violation', 'Marburg', '19 September 2014']\n"
     ]
    }
   ],
   "source": [
    "print nltk.regexp_tokenize(corpusText, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "print [(w, len(w)) for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres ', 'ris', 'es ', 'igres comen ', 'rigo en un ', 'rigal.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'Tres tristes tigres comen trigo en un trigal.'\n",
    "raw.split('t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "for char in raw[:10]:\n",
    "    print char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise 13)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres', 'tristes', 'tigres', 'comen', 'trigo', 'en', 'un', 'trigal.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres', 'tristes', 'tigres', 'comen', 'trigo', 'en', 'un', 'trigal.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres', 'tristes', 'tigres', 'comen', 'trigo', 'en', 'un', 'trigal.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Tres\\ttristes\\ttigres\\tcomen\\ttrigo\\ten\\tun\\ttrigal.'\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres\\ttristes\\ttigres\\tcomen\\ttrigo\\ten\\tun\\ttrigal.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres', 'tristes', 'tigres', 'comen', 'trigo', 'en', 'un', 'trigal.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Tres   tristes   tigres   comen   trigo   en   un   trigal.'\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres',\n",
       " '',\n",
       " '',\n",
       " 'tristes',\n",
       " '',\n",
       " '',\n",
       " 'tigres',\n",
       " '',\n",
       " '',\n",
       " 'comen',\n",
       " '',\n",
       " '',\n",
       " 'trigo',\n",
       " '',\n",
       " '',\n",
       " 'en',\n",
       " '',\n",
       " '',\n",
       " 'un',\n",
       " '',\n",
       " '',\n",
       " 'trigal.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres', 'tristes', 'tigres', 'comen', 'trigo', 'en', 'un', 'trigal.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Tres \\ttristes\\t\\t\\ttigres\\t\\t comen\\t \\t trigo en un trigal.'\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres',\n",
       " '\\ttristes\\t\\t\\ttigres\\t\\t',\n",
       " 'comen\\t',\n",
       " '\\t',\n",
       " 'trigo',\n",
       " 'en',\n",
       " 'un',\n",
       " 'trigal.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tres', 'tristes', 'tigres', 'comen', 'trigo', 'en', 'un', 'trigal.']\n",
      "['Tres', 'comen', 'en', 'tigres', 'trigal.', 'trigo', 'tristes', 'un']\n"
     ]
    }
   ],
   "source": [
    "words = raw.split()\n",
    "print words\n",
    "words.sort()\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres', 'comen', 'en', 'tigres', 'trigal.', 'trigo', 'tristes', 'un']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = raw.split()\n",
    "sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tres', 'tristes', 'tigres', 'comen', 'trigo', 'en', 'un', 'trigal.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .sort() changes original list, sorted() returns new list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 15)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3' * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('3') * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'montyTest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5317fa815d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmontyTest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'montyTest' is not defined"
     ]
    }
   ],
   "source": [
    "montyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test import montyTest\n",
    "montyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import test\n",
    "test.montyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 17)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  some superexcitingly   long example  words\n"
     ]
    }
   ],
   "source": [
    "words = ['some', 'superexcitingly', 'long', 'example', 'words']\n",
    "for w in words:\n",
    "    print '%6s' % w,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some   superexcitingly long   example words \n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print '%-6s' % w,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  some\n",
      "superexcitingly\n",
      "  long\n",
      "example\n",
      " words\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print '%6s' % w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 18)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myCorpus = load('corpus')\n",
    "tokens = nltk.wordpunct_tokenize(myCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'While', 'which', 'which', 'which', 'whether', 'which', 'whether', 'which', 'whereas', 'when', 'where', 'when', 'When', 'which', 'While', 'which', 'which', 'which', 'which', 'which', 'who', 'which', 'while', 'which', 'where', 'while', 'which', 'while', 'While', 'When', 'Whenever', 'when', 'While', 'white', 'which', 'which', 'which', 'which', 'which', 'which', 'while', 'When', 'where', 'which', 'which', 'whole', 'While', 'which', 'where']\n"
     ]
    }
   ],
   "source": [
    "whWords = [w for w in tokens if w.startswith('wh') or w.startswith('Wh')]\n",
    "print whWords[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'When', 'Whenever', 'Which', 'While', 'Why', 'what', 'whatever', 'when', 'where', 'whereas', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "print sorted(set(whWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 19)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuzzy 89\\n', 'test 2312\\n', 'foo 123\\n', 'bar 34\\n', 'baz 1']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = open('freqs.txt').readlines()\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 89], ['test', 2312], ['foo', 123], ['bar', 34], ['baz', 1]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted = [[line.split()[0], int(line.split()[1])] for line in freqs]\n",
    "splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 20)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maurice Harold Macmillan\n",
      "Montreal Laboratory\n",
      "Jan Palach\n"
     ]
    }
   ],
   "source": [
    "# extracts the topic of the article of the day of given Wikipedia Homepage\n",
    "def find_topic(url, trigger):\n",
    "    text = urllib.urlopen(url).read()\n",
    "    index = text.rfind(trigger)\n",
    "    text = text[index:]\n",
    "    title_with_markup = re.findall(r'\\<b\\>.+?\\<\\/b\\>', text)[0]\n",
    "    soup = BeautifulSoup(title_with_markup)\n",
    "    return soup.get_text()\n",
    "\n",
    "# German Wikipedia:\n",
    "print find_topic('https://de.wikipedia.org/wiki/Wikipedia:Hauptseite', '<span class=\"mw-headline\" id=\"Artikel_des_Tages\">Artikel des Tages</span>')\n",
    "\n",
    "# English Wikipedia:\n",
    "print find_topic('https://en.wikipedia.org/wiki/Main_Page', '<span class=\"mw-headline\" id=\"From_today.27s_featured_article\">From today\\'s featured article</span>')\n",
    "\n",
    "# Danish Wikipedia:\n",
    "print find_topic('https://da.wikipedia.org/wiki/Forside', '<div style=\"padding-left: 38px; color:#333;\">Ugens artikel</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 21)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'mw', u'existing', u'mw', u'mw', u'articles', u'portals', u'scientists', u'escaped', u'joined', u'moved', u'arrived', u'became', u'negotiated', u'merged', u'moved', u'opened', u'reactors', u'larger', u'followed', u'fitz', u'rubiginosa', u'email', u'articles', u'occurred', u'became', u'youngest', u'goalscorer', u'published', u'disappeared', u'sprayed', u'recolonised', u'audiobooks', u'introducing', u'composers', u'parts', u'windows', u'improved', u'articles', u'dies', u'dies', u'songwriter', u'dies', u'crashes', u'including', u'members', u'adopts', u'settlements', u'forces', u'areas', u'deaths', u'knights', u'soldiers', u'captured', u'introduced', u'rules', u'players', u'called', u'elected', u'entitled', u'nanotechnology', u'tallest', u'completed', u'anniversaries', u'email', u'anniversaries', u'varieties', u'domesticated', u'carpio', u'purposes', u'ponds', u'gardens', u'tunggal', u'pictures', u'areas', u'projects', u'resources', u'activities', u'areas', u'questions', u'using', u'languages', u'librarians', u'volunteers', u'questions', u'subjects', u'updates', u'articles', u'releases', u'discussions', u'including', u'areas', u'issues', u'policies', u'projects', u'hosted', u'hosts', u'projects', u'software', u'coordination', u'textbooks', u'manuals', u'quotations', u'materials', u'activities', u'languages', u'contains', u'articles', u'largest', u'articles', u'articles', u'bokm', u'articles', u'nynorsk', u'categories', u'existing', u'tools', u'events', u'changes', u'changes', u'pages', u'projects', u'bokm', u'nynorsk', u'srpski', u'modified', u'terms', u'using', u'trademark', u'mw']\n"
     ]
    }
   ],
   "source": [
    "def unknown(url):\n",
    "    content = getContentFromURL(url)\n",
    "    lowercased = re.findall(r'[\\s\\(\\[\\{]([a-z]+)', content)\n",
    "    words = nltk.corpus.words.words()\n",
    "    return [w for w in lowercased if w not in words]\n",
    "print unknown('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# derived forms, abbreviations, foreign words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 22)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'things', u'alerts', u'says', u'sheds', u'hats', u'recycling', u'implants', u'sites', u'services', u'businesses', u'inbox', u'allowed', u'stories', u'oldest', u'birds', u'issues', u'barcodes', u'languages', u'changing', u'waits', u'tv', u'pictures', u'seekers', u'claiming', u'officers', u'comments', u'children', u'convicted', u'condemns', u'actors', u'helps', u'stars', u'internet', u'bosses', u'weeks', u'fraudster', u'aliens', u'clues', u'haveyoursay', u'strokes', u'dies', u'earlier', u'chiefs', u'allowing', u'courts', u'doctors', u'arriving', u'benefits', u'died', u'wrestled', u'hours', u'interpretations', u'turbans', u'remarks', u'items', u'newspapers', u'introduces', u'believes', u'choices', u'descended', u'temperatures', u'scores', u'offenders', u'minutes', u'finds'])\n"
     ]
    }
   ],
   "source": [
    "print unknown('http://news.bbc.co.uk/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'senators', u'named', u'videos', u'resigns', u'hats', u'crimes', u'hairstyle', u'officers', u'arriving', u'retires', u'punched', u'farewells', u'allowed', u'cows', u'choices', u'birds', u'issues', u'languages', u'tv', u'alerts', u'girls', u'alleged', u'comments', u'children', u'parties', u'filming', u'buns', u'condemns', u'actors', u'helps', u'descended', u'memes', u'has', u'decades', u'aliens', u'shows', u'kicked', u'haveyoursay', u'seconds', u'dies', u'approves', u'earlier', u'grossing', u'robots', u'hours', u'allowing', u'sites', u'scores', u'remarks', u'died', u'hits', u'migrants', u'turbans', u'sanctions', u'charges', u'introduces', u'believes', u'overworked', u'steps', u'temperatures', u'shares', u'minutes', u'finds', u'launched', u'inbox'])\n"
     ]
    }
   ],
   "source": [
    "def unknown(url):\n",
    "    text = urllib.urlopen(url).read()\n",
    "    text = re.sub(r'\\<script(?:.|\\n)*?\\<\\/script\\>', '', text)\n",
    "    text = re.sub(r'\\<style(?:.|\\n)*?\\<\\/style\\>', '', text)\n",
    "    soup = BeautifulSoup(text)\n",
    "    content = soup.get_text()\n",
    "    lowercased = re.findall(r'[\\s\\(\\[\\{]([a-z]+)', content)\n",
    "    words = nltk.corpus.words.words()\n",
    "    return set([w for w in lowercased if w not in words])\n",
    "\n",
    "print unknown('http://www.bbc.com/news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 23)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'don', 't', 'hate', 'regular', 'expressions']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"I don't hate regular expressions.\"\n",
    "nltk.regexp_tokenize(sample_text, r'n\\'t|\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'hate', 'regular', 'expressions']\n",
      "['It', 'does', \"n't\", 'split', 'donald']\n"
     ]
    }
   ],
   "source": [
    "# doesn't work because of greediness of operators -> don matches \\w+\n",
    "print nltk.regexp_tokenize(sample_text, r'\\w+(?=n\\'t)|n\\'t|\\w+')\n",
    "print nltk.regexp_tokenize('It doesn\\'t split donald.', r'\\w+(?=n\\'t)|n\\'t|\\w+') # ?= lookahead assertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 24)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h3||0 w0r|d!\n",
      "1t 15 g3tt1ng |85w33t!\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    text = text.lower();\n",
    "    trans = [('ate', '8'), ('e', '3'), ('i', '1'), ('o', '0'), ('l', '|'), ('s', '5'), ('\\.', '5w33t!')]\n",
    "    for (key, value) in trans:\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "\n",
    "print encode('Hello World!')\n",
    "print encode('It is getting late.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$h0u|d tr3at $3a d1ff3r3nt fr0m a555w33t!'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_enhanced(text):\n",
    "    text = text.lower();\n",
    "    trans = [('ate', '8'), ('e', '3'), ('i', '1'), ('o', '0'), ('l', '|'), ('^s|(?<=\\s)s', '$'), ('s', '5'), ('\\.', '5w33t!')]\n",
    "    #?<= lookbehind assertion\n",
    "    for (key, value) in trans:\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "encode_enhanced('Should treat sea different from ass.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 25)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingstray'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "def piginizeWord(word):\n",
    "    cons = re.findall(r'^[^aeiouAEIOU]*', word)\n",
    "    return word[len(cons[0]):] + cons[0] + 'ay'\n",
    "    \n",
    "piginizeWord('string')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'omeSay uietqay ingstray erehay atthay ouldshay ebay onvertedcay otay igPay atinLay atay onceay.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b\n",
    "def piginizeText(text):\n",
    "    def helper(matchObj):\n",
    "        return piginizeWord(matchObj.group(0))\n",
    "    return re.sub(r'[A-Za-z]+', helper, text)\n",
    "piginizeText('Some quiet string here that should be converted to Pig Latin at once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ymay ietquay ellowyay ylishstay ingstray atthay ouldshay ebay onvertedcay otay Igpay Atinlay atay onceay.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c\n",
    "def piginizeWordImproved(word):\n",
    "    cons = re.findall(r'^[^aeiouAEIOU]+(?=y)|^[^aeiouqAEIOUQ]*(?:qu)?(?:Qu)?[^aeiouqAEIOUQ]*', word)[0]\n",
    "    remainder = word[len(cons):]\n",
    "    if (word.istitle()):\n",
    "        return remainder.title() + cons.lower() + 'ay'\n",
    "    return remainder + cons + 'ay'\n",
    "\n",
    "def piginizeText(text):\n",
    "    def helper(matchObj):\n",
    "        return piginizeWordImproved(matchObj.group(0))\n",
    "    return re.sub(r'[A-Za-z]+', helper, text)\n",
    "piginizeText('My quiet yellow stylish string that should be converted to Pig Latin at once.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 26)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = urllib.urlopen('https://tr.wikipedia.org/wiki/%C4%B0stanbul').read()\n",
    "text = re.sub(r'\\<script(?:.|\\n)*?\\<\\/script\\>', '', text)\n",
    "text = re.sub(r'\\<style(?:.|\\n)*?\\<\\/style\\>', '', text)\n",
    "soup = BeautifulSoup(text)\n",
    "content = soup.get_text()\n",
    "tokens = nltk.wordpunct_tokenize(content)\n",
    "text = nltk.Text(tokens)\n",
    "words = [w.lower() for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'iau', u'iiei', u'iau', u'iiei', u'aioei', u'aa', u'aiie', u'oo', u'eii', u'aa', u'eieei', u'ieee', u'eee', u'eiii', u'uuaa', u'ouu', u'uaa', u'aa', u'ua', u'aa', u'a', u'ie', u'aaa', u'ii', u'iau', u'aa', u'a', u'aaa', u'a', u'ooiaa', u'e', u'iau', u'i', u'ei', u'uaa', u'aaa', u'aa', u'e', u'aii', u'aaa', u'uei', u'iia', u'aei', u'e', u'i', u'aa', u'ee', u'i', u'eeei', u'oa']\n"
     ]
    }
   ],
   "source": [
    "vowel_sequences = []\n",
    "for word in words:\n",
    "    vowels = ''.join(re.findall(r'[aeiou]', word))\n",
    "    if (len(vowels) > 0):\n",
    "        vowel_sequences.append(vowels)\n",
    "print vowel_sequences[:50]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'i', u'a'), (u'a', u'u'), (u'i', u'i'), (u'i', u'e'), (u'e', u'i'), (u'i', u'a'), (u'a', u'u'), (u'i', u'i'), (u'i', u'e'), (u'e', u'i'), (u'a', u'i'), (u'i', u'o'), (u'o', u'e'), (u'e', u'i'), (u'a', u'a'), (u'a', u'i'), (u'i', u'i'), (u'i', u'e'), (u'o', u'o'), (u'e', u'i'), (u'i', u'i'), (u'a', u'a'), (u'e', u'i'), (u'i', u'e'), (u'e', u'e'), (u'e', u'i'), (u'i', u'e'), (u'e', u'e'), (u'e', u'e'), (u'e', u'e'), (u'e', u'e'), (u'e', u'i'), (u'i', u'i'), (u'i', u'i'), (u'u', u'u'), (u'u', u'a'), (u'a', u'a'), (u'o', u'u'), (u'u', u'u'), (u'u', u'a'), (u'a', u'a'), (u'a', u'a'), (u'u', u'a'), (u'a', u'a'), (u'i', u'e'), (u'a', u'a'), (u'a', u'a'), (u'i', u'i'), (u'i', u'a'), (u'a', u'u')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = []\n",
    "for vowel_seq in vowel_sequences:\n",
    "    count = 0\n",
    "    while (count + 1 < len(vowel_seq)):\n",
    "        bigrams.append((vowel_seq[count], vowel_seq[count + 1]))\n",
    "        count += 1\n",
    "print bigrams[:50]                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i', u'a', u'e', u'u', u'o']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    e    i    o    u \n",
      "a 4813  786 1975  355  880 \n",
      "e  506 1805 3821  245  207 \n",
      "i 1658 2884 2539  297   59 \n",
      "o 1048  215  221  287  672 \n",
      "u  900  154  174   50  860 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=vowels,samples=vowels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 27)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hahhhhehea hhaa ah ehahaahe hhhhheahhhahehe ahheheea hheaeheeah hh eehahaaahhhh h e hehe aehee ehheeh hh heae hheae e a ah eaheeehhh h hh h ehhh heeehhhehhhehahhahhh haee aheheeehhahe ahaeheee e ehhaeehaahaeeehahhehehee haah ah a haaheaea eh a h hea e hahheaahh aehaeeh ahh e ahhea eea hhhh eaa hhhe a hhhh aehhehhahhhehaea eh haa eeaeh hheahe ahaah hahe h hah hhhahheahh eh haehhheehh eh ehhh a ahahhhhhhh ehhhaa aa ehe heh ahahhhaeehhh hh hhehhheehhehhae ah ahae ehheh ee hhhhahha e'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def laugh():\n",
    "    raw = ''.join(random.choice('aehh ') for x in range(500))\n",
    "    return ' '.join(raw.split())\n",
    "laugh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 28)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# three words -> woulld be compatible with splitting on whitespace\n",
    "# one compound word -> would make sense semantically, may be relevant for natural language understanding applications\n",
    "# nine words -> would make sense phonetically, relevant for speech processing applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 29)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2547561971\n",
      "11.9260070433\n",
      "12.084303495\n",
      "4.34922419804\n"
     ]
    }
   ],
   "source": [
    "def ari(category):\n",
    "    words = nltk.corpus.brown.words(categories=category)\n",
    "    sents = nltk.corpus.brown.sents(categories=category)\n",
    "    av_wordlength = sum(len(w) for w in words) / len(words)\n",
    "    av_sentlength = sum(len(s) for s in sents) / len(sents)\n",
    "    return (4.71 * av_wordlength) + (0.5 * av_sentlength) - 21.43\n",
    "print ari('lore')\n",
    "print ari('learned')\n",
    "print ari('government')\n",
    "print ari('romance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 30)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'New', u'rule', u'allow', u'Sikh', u'polic', u'offic', u'to', u'wear', u'turban', u'instead', u'of', u'tradit', u'polic', u'hat', u'have', u'been', u'introduc', u'in', u'New', u'York', u',', u'offici', u'say', u'.', u'The', u'New', u'York', u'Polic', u'Depart', u'said', u'the', u'turban', u'must', u'be', u'navi', u'blue', u'and', u'have', u'the', u'NYPD', u'insignia', u'attach', u'.', u'Under', u'the', u'new', u'rule', u',', u'religi', u'member', u'of', u'the', u'forc', u'are', u'also', u'permit', u'to', u'grow', u'beard', u'up', u'to', u'half', u'-', u'an', u'-', u'inch', u'long', u'.', u'Sikh', u'offic', u'have', u'until', u'now', u'worn', u'turban', u'under', u'their', u'cap', u'.', u'Beard', u'have', u'not', u'been', u'permit', u'.']\n",
      "\n",
      "\n",
      "\n",
      "['new', 'rul', 'allow', 'sikh', 'pol', 'off', 'to', 'wear', 'turb', 'instead', 'of', 'tradit', 'pol', 'hat', 'hav', 'been', 'introduc', 'in', 'new', 'york', ',', 'off', 'say', '.', 'the', 'new', 'york', 'pol', 'depart', 'said', 'the', 'turb', 'must', 'be', 'navy', 'blu', 'and', 'hav', 'the', 'nypd', 'insign', 'attach', '.', 'und', 'the', 'new', 'rul', ',', u'religy', 'memb', 'of', 'the', 'forc', 'ar', 'also', 'permit', 'to', 'grow', 'beard', 'up', 'to', 'half', '-', 'an', '-', 'inch', 'long', '.', 'sikh', 'off', 'hav', 'until', 'now', 'worn', 'turb', 'und', 'their', 'cap', '.', 'beard', 'hav', 'not', 'been', 'permit', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "text = 'New rules allowing Sikh police officers to wear turbans instead of traditional police hats have been introduced in New York, officials say. The New York Police Department said the turbans must be navy blue and have the NYPD insignia attached. Under the new rules, religious members of the force are also permitted to grow beards up to half-an-inch long. Sikh officers have until now worn turbans under their caps. Beards have not been permitted.'\n",
    "tokens = nltk.wordpunct_tokenize(text)\n",
    "print [porter.stem(t) for t in tokens]\n",
    "print '\\n\\n'\n",
    "print [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Porter preserves upper case, uses unicode, seems to tend to longer stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 31)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "lengths = []\n",
    "for w in saying:\n",
    "    lengths.append(len(w))\n",
    "lengths    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 32)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "# a\n",
    "bland = silly.split()\n",
    "print bland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b\n",
    "''.join(w[1] for w in bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c\n",
    "' '.join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "# d\n",
    "for w in sorted(bland):\n",
    "    print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 33)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b\n",
    "words = ['this', 'is', 'a', 'dull', 'list', 'of', 'words']\n",
    "words.index('dull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c\n",
    "bland[:bland.index('in')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 34)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada\n",
      "Australia\n",
      "China\n"
     ]
    }
   ],
   "source": [
    "def convertNationality(adjective):\n",
    "    if (adjective.endswith('dian') or adjective.endswith('ese')):\n",
    "        return adjective[:-3] + 'a'\n",
    "    elif (adjective.endswith('ian')):\n",
    "        return adjective[:-1]\n",
    "        \n",
    "print convertNationality('Canadian')   \n",
    "print convertNationality('Australian')\n",
    "print convertNationality('Chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 35)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n",
      "[u'as best you can']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "pronouns = ['I', 'you', 'he', 'she', 'it', 'we', 'they']\n",
    "corpus = ' '.join(nltk.corpus.webtext.words())\n",
    "sample1 = re.findall(r'[aA]s best as (?:I|you|he|she|it|we|they) can', corpus)\n",
    "print sample1[:10]\n",
    "print len(sample1)\n",
    "sample2 = re.findall(r'[aA]s best (?:I|you|he|she|it|we|they) can', corpus)\n",
    "print sample2[:10]\n",
    "print len(sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 36)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh hai . In teh beginnin Ceiling Cat maded teh skiez An da Urfs , but he did not eated dem . Da Urfs no had shapez An haded dark face , An Ceiling Cat rode invisible bike over teh waterz . An Ceiling Cat sayed light Day An dark no Day . It were FURST !!! 1 An Ceiling Cat sayed , i can has teh firmmint wich iz funny bibel naim 4 ceiling , so wuz teh twoth day . An Ceiling Cat called no waterz urth and waters oshun . Iz good . An so teh threeth day jazzhands . An so teh furth day w00t . An so teh ... fith day . Ceiling Cat taek a wile 2 cawnt . An Ceiling Cat doed moar living stuff , mooes , An creepies , An otehr animuls , An did not eated tehm . An Ceiling Cat sayed , letz us do peeps like uz , becuz we ish teh qte , An let min p0wnz0r becuz tehy has can openers . So Ceiling Cat createded teh peeps taht waz like him , can has can openers he maed tehm , min An womin wuz maeded , but he did not eated tehm . An Ceiling Cat sed them O hai maek bebehs kthx , An p0wn teh waterz , no waterz An teh firmmint , An evry stufs . For evry createded stufs tehre are the fuudz , to the burdies , teh creepiez , An teh mooes , so tehre . It happen . Iz good . An Ceiling Cat sayed , Beholdt , teh good enouf for releaze as version 0 . 8a . kthxbai . An teh skyz an teh Urfs wur finishd , an al teh stufz in dem , an Ceiling Cat was liek al tired an stuf . Ceiling Cat blesd teh 7f day , an sed itz teh h0liez0rz ; cuz dats when he restd fum all his werk wich Ceiling Cat creatd an maed . Yay holy Caturday ! Iz how teh skyz an Urfs wur maed , wen Ceiling Cat pwnt . Urfs no can has plantz n treez n catnipz yet , cuz Ceiling Cat no can maek rainz , but iz ok for kittehs DUNT LYKEZ wetfurz . An ther wuznt ne man to mek farmz n stuf ; cuz teh clowds wur al happie an dint feel liek cryin , wich wuz ok to cuz umbrellaz wuznt inventd yut . An Ceiling Cat madez kitteh owt ov teh flore dust , an breathd ntew his nawstrils teh bref ov life , wich wuz sorta liek doin cpr on a mudpie , but it wuz al gud . An Ceiling Cat madez evry tre dat iz prity , an gud fur fud ; teh tre ov lief wuz in teh gardun to , an teh tre ov teh nawlej ov gud an evul . man askd Ceiling Cat to makez a kooki tree ,\n"
     ]
    }
   ],
   "source": [
    "print ' '.join(nltk.corpus.genesis.words('lolcat.txt')[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai\n",
      "hai\n",
      "riet\n",
      "kiet\n",
      "liek\n",
      "ovah\n",
      "lowd\n",
      "kitteh\n",
      "free\n",
      "nofin\n",
      "littel\n"
     ]
    }
   ],
   "source": [
    "def lolcat(word):\n",
    "    word = re.sub(r'ight', 'iet', word)\n",
    "    word = re.sub(r'^I$', 'ai', word)\n",
    "    word = re.sub(r'(?<=[^aeiouAEIOU])i$', 'ai', word)\n",
    "    word = re.sub(r'le$', 'el', word)\n",
    "    def helper(matchObj):\n",
    "        return 'e' + matchObj.group(1)\n",
    "    word = re.sub(r'([^aeiouAEIOU])e$', helper, word)\n",
    "    word = re.sub(r'(?<=[^aeiouAEIOU])er$', 'ah', word)\n",
    "    word = re.sub(r'ou', 'ow', word)\n",
    "    word = re.sub(r'Ou', 'Ow', word)\n",
    "    word = re.sub(r'(?<=[^aeiouAEIOU])y$', 'eh', word)\n",
    "    word = re.sub(r'th', 'f', word)\n",
    "    word = re.sub(r'Th', 'F', word)\n",
    "    word = re.sub(r'ing$', 'in', word)\n",
    "    return word    \n",
    "print lolcat('I')\n",
    "print lolcat('hi')\n",
    "print lolcat('right')\n",
    "print lolcat('kite')\n",
    "print lolcat('like')\n",
    "print lolcat('over')\n",
    "print lolcat('loud')\n",
    "print lolcat('kitty')\n",
    "print lolcat('three')\n",
    "print lolcat('nothing')\n",
    "print lolcat('little')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 37)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A span which should be cleaned'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(html):\n",
    "    # remove html tags:\n",
    "    text = re.sub(r'\\<.*?\\>', '', html)\n",
    "    # normalize whitespace:\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "clean('<span class=\"some class\">A span    which  should<br> be cleaned</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 38)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long-\\nterm', 'encyclo-\\npedia']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "text = 'some text with long-\\nterm and encyclo-\\npedia'\n",
    "words = re.findall(r'\\w+\\-\\n\\w+', text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long-term\n",
      "encyclo-pedia\n"
     ]
    }
   ],
   "source": [
    "# b\n",
    "for w in words:\n",
    "    print re.sub('\\n', '', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long-term\n",
      "encyclopedia\n"
     ]
    }
   ],
   "source": [
    "# c\n",
    "for w in words:\n",
    "    word = re.sub('\\n', '', w)\n",
    "    parts = word.lower().split('-')\n",
    "    if (parts[0] not in nltk.corpus.words.words() and parts[1] not in nltk.corpus.words.words()):\n",
    "        print re.sub('\\-', '', word)\n",
    "    else:\n",
    "        print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 39)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R163\n",
      "R163\n",
      "R150\n",
      "A261\n",
      "A261\n",
      "T522\n",
      "P236\n"
     ]
    }
   ],
   "source": [
    "def soundex(name):\n",
    "    first = name[0]\n",
    "    # remove w & h\n",
    "    encoded = first.lower() + re.sub('[wh]', '', name[1:].lower())\n",
    "    # replace consonants with numbers\n",
    "    encoded = re.sub(r'[bfpv]', '1', encoded)\n",
    "    encoded = re.sub(r'[cgjkqsxz]', '2', encoded)\n",
    "    encoded = re.sub(r'[dt]', '3', encoded)\n",
    "    encoded = re.sub(r'l', '4', encoded)\n",
    "    encoded = re.sub(r'[mn]', '5', encoded)\n",
    "    encoded = re.sub(r'r', '6', encoded)\n",
    "    # merge adjacent same digits into one\n",
    "    count = 1\n",
    "    while count < 7:\n",
    "        encoded = re.sub(str(count) + '{2,}', str(count), encoded)\n",
    "        count += 1\n",
    "    # remove vowels\n",
    "    encoded = encoded[0].upper() + re.sub('[aeiouy]', '', encoded[1:])\n",
    "    # if first character is digit, replace it with the saved letter\n",
    "    if (encoded[0].isdigit()):\n",
    "        encoded = first.upper() + encoded[1:]\n",
    "    # encoded must contain 3 digits -> fill it up with zeros if too short    \n",
    "    if (len(encoded) < 4):\n",
    "        encoded += '000'\n",
    "    return encoded[:4]    \n",
    "    \n",
    "print soundex('Robert') #R163\n",
    "print soundex('Rupert') #R163\n",
    "print soundex('Rubin') #R150\n",
    "print soundex('Ashcraft') #A261\n",
    "print soundex('Ashcroft') #A261\n",
    "print soundex('Tymczak') #T522 \n",
    "print soundex('Pfister') #P236   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 40)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.5700668936\n",
      "68.8154763376\n"
     ]
    }
   ],
   "source": [
    "def ari(raw):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sents = sent_tokenizer.tokenize(raw)\n",
    "    words = nltk.word_tokenize(raw)\n",
    "    av_wordlength = sum(len(w) for w in words) / len(words)\n",
    "    av_sentlength = sum(len(s) for s in sents) / len(sents)\n",
    "    return (4.71 * av_wordlength) + (0.5 * av_sentlength) - 21.43\n",
    "print ari(nltk.corpus.abc.raw(\"rural.txt\"))\n",
    "print ari(nltk.corpus.abc.raw(\"science.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 41)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
    "# more elegant with regular expression instead of nested list comprehension:\n",
    "vsequences = set([''.join(re.findall(r'[aeiou]', word)) for word in words])\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nested list comprehension:\n",
    "vsequences = set([''.join([char for char in word if char in 'aeiou']) for word in words])\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 42)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no (WordNet Offset: 751944)\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of (WordNet Offset: 6756831)\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !    (WordNet Offset: 6756831)\n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well   (WordNet Offset: 6756831)\n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which  (WordNet Offset: 6756831)\n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog -- (WordNet Offset: 6756831)\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k (WordNet Offset: 6756831)\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t (WordNet Offset: 6756831)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "class IndexedText(object):\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "            for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4) # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            offset = '(WordNet Offset: ' + str(wn.synsets(self._text[i])[0].offset()) + ')'\n",
    "            ldisplay = '%*s' % (width, lcontext[-width:])\n",
    "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
    "            print ldisplay, rdisplay, offset\n",
    "                \n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 43)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function spearman_correlation in module nltk.metrics.spearman:\n",
      "\n",
      "spearman_correlation(ranks1, ranks2)\n",
      "    Returns the Spearman correlation coefficient for two rankings, which\n",
      "    should be dicts or sequences of (key, rank). The coefficient ranges from\n",
      "    -1.0 (ranks are opposite) to 1.0 (ranks are identical), and is only\n",
      "    calculated for keys in both rankings (for meaningful results, remove keys\n",
      "    present in only one list before ranking).\n",
      "\n",
      "English\n",
      "German_Deutsch\n"
     ]
    }
   ],
   "source": [
    "def guessLanguage(text):\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    text = nltk.Text(tokens)\n",
    "    fdist_text = nltk.FreqDist(text)\n",
    "    best_guess = ('', 0)\n",
    "    best_intersection = []\n",
    "    for lang in nltk.corpus.udhr.fileids():\n",
    "        if (lang.endswith('-Latin1')):\n",
    "            fdist_lang = nltk.FreqDist(nltk.corpus.udhr.words(lang))\n",
    "            intersection = list(set(fdist_text.keys()) & set(fdist_lang.keys()))\n",
    "            dict_text = []\n",
    "            dict_lang = []\n",
    "            for word in intersection:\n",
    "                dict_text.append((word, fdist_text[word]))\n",
    "                dict_lang.append((word, fdist_lang[word]))\n",
    "            spearman = nltk.spearman_correlation(dict_text, dict_lang)\n",
    "            if ((best_guess[1] == 0 and spearman != 0.0) or (spearman != 0.0 and spearman > best_guess[1])):\n",
    "                best_guess = (lang[:-7], spearman)\n",
    "    return best_guess[0];\n",
    "\n",
    "help(nltk.spearman_correlation)\n",
    "print guessLanguage('This is clearly an example of English text which should not be hard to recognize.')\n",
    "print guessLanguage(u'Carapax (von gr. charax Befestigungsanlage, Palisade und pagios fest; Plural: Carapaces) ist eine Bezeichnung fr eine bei verschiedenen Tiergruppen (Taxa) unabhngig voneinander entstandene harte Bedeckung der Krperoberseite. Bei Schildkrten heit der Carapax gemeinsprachlich Rckenschild oder Rckenpanzer, bei Krustentieren (Krebstieren in der Kche) ist er ein Teil der Schale. Viele Krebstiere (Crustacea) besitzen eine Hautfalte, die vom Kopfhinterrand (Segment der 2. Maxille) ausgeht; diese kann auch primr (z. B. Cephalocarida) oder sekundr (z. B. Asseln und Flohkrebse) fehlen, gehrt also nicht zum Grundbauplan der Krebstiere. Vielfach ist die chitinse Kopffalte durch eingelagerten Kalk panzerartig versteift, vor allem bei vielen Zehnfukrebsen. Bedeckt diese Struktur als Rckenschild einige oder ggf. alle Rumpfsegmente, wird sie Carapax genannt. Der Carapax schliet also an den Kopf an, setzt sich ber dessen Hinterrand hinaus fort und erstreckt sich mehr oder weniger weit ber den Rumpf des Krebses. Je nach Ausbildung kann er auch den Kopf selbst umhllen (z. B. bei den Muschelkrebsen) und mehr oder weniger weit auch seitlich herabgezogen sein.   Zum Artikel ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish_Dansk\n"
     ]
    }
   ],
   "source": [
    "print guessLanguage(u'Ddsstraf eller livsstraf er henrettelse som straf for en forbrydelse. I de jurisdiktioner, der praktiserer ddsstraf, er den som regel forbeholdt et lille antal alvorlige forbrydelser, ofte overlagt mord og landsforrderi. I Kina praktiseres tillige ddsstraf for konomisk kriminalitet og narkokriminalitet, og i Iran for homoseksualitet, ligesom der i visse omrder kontrolleret af islamiske oprrsbevgelser gennemfres henrettelser baseret p en streng fortolkning af sharia. Mange lande har ddsstraf i den militre straffelov eller for forbrydelser beget i krigstid. I Danmark blev ddsstraf frste gang afskaffet i den borgerlige straffelov den 15. april 1930. Loven trdte i kraft 15. april 1933. Ddsstraf blev p dette tidspunkt beholdt i den militre straffelov. I forbindelse med retsopgret efter 2. verdenskrig genindfrtes ddsstraffen (som kaldtes livsstraf) i 1945 for forbrydelser beget under besttelsen. Loven var en srlov og kendes som Landsforrderloven eller retteligen Straffelovstillgget og havde tilbagevirkende kraft for handlinger beget efter 9. april 1940. 46 personer blev p den baggrund henrettet af frivillige politifolk. Den 20. juli 1950 kl. 01:00 blev Ib Birkedal Hansen henrettet som den sidste i Danmark. (Ls mere..)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 44)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Jane', u'Fairfax', u'therefore', u'that', u'he', u'would', u'have', u'preferred', u'the', u'society', u'of', u'William', u'Larkins', u'.', u'No', u'!--', u'she', u'was', u'more', u'and', u'more', u'convinced', u'that', u'Mrs', u'.', u'Weston', u'was', u'quite', u'mistaken', u'in', u'that', u'surmise', u'.', u'There', u'was', u'a', u'great', u'deal', u'of', u'friendly', u'and', u'of', u'compassionate', u'attachment', u'on', u'his', u'side', u'--', u'but', u'no', u'love', u'.', u'Alas', u'!', u'there', u'was', u'soon', u'no', u'leisure', u'for', u'quarrelling', u'with', u'Mr', u'.', u'Knightley', u'.', u'Two', u'days', u'of', u'joyful', u'security', u'were', u'immediately', u'followed', u'by', u'the', u'over', u'-', u'throw', u'of', u'every', u'thing', u'.', u'A', u'letter', u'arrived', u'from', u'Mr', u'.', u'Churchill', u'to', u'urge', u'his', u'nephew', u\"'\", u's', u'instant', u'return', u'.', u'Mrs']\n",
      "Average Similarity:  0.12389011066\n"
     ]
    }
   ],
   "source": [
    "def novel_sense(word, text):\n",
    "    content_words = []\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    count = 0\n",
    "    for w in text:\n",
    "        if (w.isalpha() and w not in stopwords):\n",
    "            content_words.append((w, count))\n",
    "        count += 1    \n",
    "    count = 0\n",
    "    oddest = False\n",
    "    for w in content_words:\n",
    "        if (w[0] == word):\n",
    "            count_comparisons = 0\n",
    "            overall_sim = 0\n",
    "            for synset in wn.synsets(w[0]):\n",
    "                # compare to words in context on left side:\n",
    "                for index in range(1, min(21, count+1)):\n",
    "                    context_word = content_words[count - index][0]\n",
    "                    for context_synset in wn.synsets(context_word):\n",
    "                        path_sim = synset.path_similarity(context_synset)\n",
    "                        if (path_sim != None):\n",
    "                            overall_sim += path_sim \n",
    "                            count_comparisons += 1\n",
    "                # compare to words in context on right side:            \n",
    "                for index in range(1, min(21, len(content_words)-count-1)):\n",
    "                        context_word = content_words[count + index][0]\n",
    "                        for context_synset in wn.synsets(context_word):\n",
    "                            path_sim = synset.path_similarity(context_synset)\n",
    "                            if (path_sim != None):\n",
    "                                overall_sim += path_sim \n",
    "                                count_comparisons += 1            \n",
    "            av_sim = overall_sim / count_comparisons\n",
    "            if (oddest == False or oddest[1] > av_sim):\n",
    "                oddest = (w[1], av_sim) # w[1] = original index of the word in the text\n",
    "        count += 1\n",
    "    if (oddest != False):    \n",
    "        print text[max(0, oddest[0] - 50):min(oddest[0] + 50, len(text))]\n",
    "        print 'Average Similarity: ', str(oddest[1])\n",
    "\n",
    "novel_sense('love', nltk.corpus.gutenberg.words('austen-emma.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
